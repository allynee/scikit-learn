{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b86bc9a",
   "metadata": {},
   "source": [
    "# 1. Creating Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6adebe",
   "metadata": {},
   "source": [
    "### a) Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1b6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create X from the radio column's values\n",
    "X = sales_df[\"radio\"].values\n",
    "\n",
    "# Create y from the sales column's values\n",
    "y = sales_df[\"sales\"].values\n",
    "\n",
    "# Reshape X\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "# Check the shape of the features and targets\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce002e51",
   "metadata": {},
   "source": [
    "### b) Splitting Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e45512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y arrays\n",
    "X = sales_df.drop(\"sales\", axis=1).values\n",
    "y = sales_df[\"sales\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560eb981",
   "metadata": {},
   "source": [
    "# 2. Linear Regression Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1d3ce",
   "metadata": {},
   "source": [
    "### a) Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6250f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# Create the model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "reg.fit(X,y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = reg.predict(X)\n",
    "\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f292a",
   "metadata": {},
   "source": [
    "### b) Using Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "reg.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Predictions: {}, Actual Values: {}\".format(y_pred[:2], y_test[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d956e",
   "metadata": {},
   "source": [
    "# 3. Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(X, y, color=\"blue\")\n",
    "\n",
    "# Create line plot\n",
    "plt.plot(X, predictions, color=\"red\")\n",
    "plt.xlabel(\"Radio Expenditure ($)\")\n",
    "plt.ylabel(\"Sales ($)\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8831d60",
   "metadata": {},
   "source": [
    "# 4. Testing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf2cbb",
   "metadata": {},
   "source": [
    "### a) R-squared and mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7bb076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Compute R-squared\n",
    "r_squared = reg.score(X_test, y_test)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"R^2: {}\".format(r_squared))\n",
    "print(\"RMSE: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94d0a9",
   "metadata": {},
   "source": [
    "### mean_squared_error vs mean_absolute_error:\n",
    "\n",
    "To put it in short, if there are many outliers then you may consider using Mean Absolute Error (also called the Average Absolute Deviation). RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.\n",
    "\n",
    "Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. MAE corresponds to the l1 norm or Manhattan norm while RMSE corresponds to the l2 norm or Euclidian Norm. The higher the norm index, the more it focuses on large values and neglects small ones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17f53846",
   "metadata": {},
   "source": [
    "### b) Cross Validation\n",
    "Basics:\n",
    "![Screenshot](Cross-validation.png)\n",
    "\n",
    "- 5 folds -> 5-fold CV\n",
    "- k folds -> k-fold CV\n",
    "- ...\n",
    "- More folds, more computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c3748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "#Â Create a KFold object\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Compute 6-fold cross-validation scores\n",
    "cv_scores = cross_val_score(reg, X, y, cv=kf)\n",
    "\n",
    "# Print scores\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc31ae5",
   "metadata": {},
   "source": [
    "Note: we assign a seed to random_state kw argument to make our data would b split in same way. makes results repeatable downstream.\n",
    "\n",
    "Now, we will analyze our cross-validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean\n",
    "print(np.mean(cv_results))\n",
    "\n",
    "# Print the standard deviation\n",
    "print(np.std(cv_results))\n",
    "\n",
    "# Print the 95% confidence interval\n",
    "print(np.quantile(cv_results, [0.025, 0.975]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999a467",
   "metadata": {},
   "source": [
    "### c) Regularization\n",
    "\\>> Technique to avoid over-fitting\n",
    "\n",
    "##### Why regularize?\n",
    "- Linear Reg minimize loss f(x)\n",
    "- It chooses coeff a for each variable, & b\n",
    "- Large coeffs can lead to overfitting\n",
    "- Regularization: Penalize large coeffs\n",
    "\n",
    "#### 1. Ridge Regression\n",
    "- Loss f(x) = Ordinary Least Squares (OLS) loss f(x) + sum of alpha * coeffs a \n",
    "- Alpha: Parameter we nd to choose. Similar to picking k in KNN\n",
    "- a = 0 = OLS -> Overfitting\n",
    "- Large a -> Underfitting\n",
    "\n",
    "#### 2. Lasso Regression\n",
    "- Same, loss f(x) = OLS + sum of alpha * coeffs a \n",
    "- Select impt features in dataset\n",
    "- Shrinks coeffs of less impt features to 0\n",
    "- Features not shrunk to 0 will be selected by lasso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
